{"metadata":{"colab":{"provenance":[]},"gpuClass":"premium","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7305056,"sourceType":"datasetVersion","datasetId":4238418},{"sourceId":7305400,"sourceType":"datasetVersion","datasetId":4238634},{"sourceId":7305441,"sourceType":"datasetVersion","datasetId":4036691}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q pyspellchecker","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:36:27.787740Z","iopub.execute_input":"2023-12-30T16:36:27.788322Z","iopub.status.idle":"2023-12-30T16:36:41.063113Z","shell.execute_reply.started":"2023-12-30T16:36:27.788287Z","shell.execute_reply":"2023-12-30T16:36:41.062050Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re \nimport string \nimport warnings\nfrom spellchecker import SpellChecker\n\nwarnings.filterwarnings(\"ignore\")\n\n\nimport torch\nimport os\n\nfrom PIL import Image\n\nimport json\nimport matplotlib.pyplot as plt\n\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom time import sleep\nimport warnings\nfrom transformers import AdamW\nimport spacy\n\nfrom transformers import CLIPProcessor, CLIPModel\nimport torch.nn.functional as F\n\ndevice = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nwarnings.filterwarnings(\"ignore\")\n\nnlp = spacy.load(\"en_core_web_sm\")","metadata":{"id":"62eDxz05BzPC","execution":{"iopub.status.busy":"2023-12-30T16:37:42.121791Z","iopub.execute_input":"2023-12-30T16:37:42.122173Z","iopub.status.idle":"2023-12-30T16:37:51.628485Z","shell.execute_reply.started":"2023-12-30T16:37:42.122142Z","shell.execute_reply":"2023-12-30T16:37:51.627678Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# read annotations file\n\nTask1_Train = pd.read_csv(\"/kaggle/input/subtask-b/individual_stB_train.csv\")\nTask1_Dev = pd.read_csv(\"/kaggle/input/subtask-b/stB_eval.csv\")\nTask1_Test = pd.read_csv(\"/kaggle/input/subtask-b/stB_test.csv\")\n\nTask1_Train = Task1_Train.reset_index()\n\nTask1_Train['ORG'] = 0\nTask1_Train['PER'] = 0\nTask1_Train['NOORG'] = 0\n\nTask1_Test['ORG'] = 0\nTask1_Test['PER'] = 0\nTask1_Test['NOORG'] = 0\n\nTask1_Train.dropna(inplace=True)","metadata":{"id":"AunCHFubBxUO","execution":{"iopub.status.busy":"2023-12-30T16:37:55.091708Z","iopub.execute_input":"2023-12-30T16:37:55.092337Z","iopub.status.idle":"2023-12-30T16:37:55.140975Z","shell.execute_reply.started":"2023-12-30T16:37:55.092304Z","shell.execute_reply":"2023-12-30T16:37:55.140149Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def remove_emojis(text):\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n                               u\"\\U0001F700-\\U0001F77F\"  # Alphanumeric Supplement\n                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n                               u\"\\U000024C2-\\U0001F251\" \n                               \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_punctuation(input_string):\n    translator = str.maketrans('', '', string.punctuation)\n    \n    result = input_string.translate(translator)\n    \n    return result\n\ndef remove_non_english_chars(input_string):\n    translation_table = dict.fromkeys(\n        i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('C')\n    )\n\n    result = input_string.translate(translation_table)\n\n    return result\n\ndef correct_spelling(input_text):\n    try:\n        # Create a SpellChecker object\n        spell = SpellChecker()\n\n        # Split the input text into words\n        words = input_text.split()\n\n        # Find misspelled words\n        misspelled = spell.unknown(words)\n\n        # Correct misspelled words\n        corrected_text = \" \".join(spell.correction(word) if word in misspelled else word for word in words)\n\n        return corrected_text\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return input_text\n    \nhyperlink_pattern = r'https?://\\S+|www\\.\\S+'\n\ndef PreProcessTweets(tweet):\n  tweet2 = str(tweet)\n  tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n  tweet2 = re.sub(r'#([^\\s]+)', '', tweet2)\n  tweet2 = tweet2.replace(\"LINK\",\"\")\n  tweet2 = tweet2.replace(\"&amp\",\"\") \n  tweet2 = re.sub(r'@', '', tweet2)\n  tweet2 = tweet2.replace(\"\\n\",\"\")\n  tweet2 = re.sub(r'_', ' ', tweet2)\n  tweet2 = tweet2.replace(\"UN\",\"United Nations \")\n  tweet2 = re.sub(hyperlink_pattern, '', tweet2)\n  tweet2 = tweet2.replace(\":\",\"\") \n  tweet2 = re.sub('‼‼‼','', tweet2)\n  tweet2 = remove_emojis(tweet2)\n  tweet2 = remove_punctuation(tweet2)\n  tweet2 = re.sub(' +', ' ',tweet2)\n  tweet2 = tweet2.replace(\"youve\",\"you have\")\n  tweet2 = tweet2.replace(\"govt\",\"Government\")\n  #tweet2 = correct_spelling(tweet2)\n  return tweet2\n\nTask1_Train['text'] = Task1_Train['text'].apply(lambda x: PreProcessTweets(x))\nTask1_Dev['text'] = Task1_Dev['text'].apply(lambda x: PreProcessTweets(x))\nTask1_Test['text'] = Task1_Test['text'].apply(lambda x: PreProcessTweets(x))","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:37:57.434644Z","iopub.execute_input":"2023-12-30T16:37:57.435073Z","iopub.status.idle":"2023-12-30T16:37:57.627194Z","shell.execute_reply.started":"2023-12-30T16:37:57.435037Z","shell.execute_reply":"2023-12-30T16:37:57.626343Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"ImagePathTest = '/kaggle/input/subtask-b/TestData/subtaskB/'\n\nfor i in range(len(Task1_Test)):\n    Task1_Test['filename'][i] = ImagePathTest + Task1_Test['filename'][i].split('/')[-1]","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:38:00.684111Z","iopub.execute_input":"2023-12-30T16:38:00.685187Z","iopub.status.idle":"2023-12-30T16:38:00.771459Z","shell.execute_reply.started":"2023-12-30T16:38:00.685148Z","shell.execute_reply":"2023-12-30T16:38:00.770262Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"Indvidual = '/kaggle/input/subtask-b/subTaskB-20231121T151628Z-001/subTaskB/Individual/'\nCommunity = '/kaggle/input/subtask-b/subTaskB-20231121T151628Z-001/subTaskB/Community/'\nOrganization = '/kaggle/input/subtask-b/subTaskB-20231121T151628Z-001/subTaskB/Organization/'\n\n# HatePath = '/kaggle/input/task-a/subTaskA-20231121T171317Z-001/subTaskA/Hate Speech/'\n# NonHatePath = '/kaggle/input/task-a/subTaskA-20231121T171317Z-001/subTaskA/No Hate Speech/'\n\nfor i in range(len(Task1_Train)):\n    if(Task1_Train['label'][i] == 0):\n        Task1_Train['filename'][i] = Indvidual + Task1_Train['filename'][i].split('/')[-1]\n    elif(Task1_Train['label'][i] == 1):\n        Task1_Train['filename'][i] = Community + Task1_Train['filename'][i].split('/')[-1]\n    else:\n        Task1_Train['filename'][i] = Organization + Task1_Train['filename'][i].split('/')[-1]\n#     elif(Task1_Train['label'][i] == 2):\n#         Task1_Train['filename'][i] = Organization + Task1_Train['filename'][i].split('/')[-1]\n \n# DevPath = '/kaggle/input/subtask-b/subtaskB-20231121T151609Z-001/subtaskB/'\n\n# for i in range(len(Task1_Dev)):\n#     if(Task1_Dev['label'][i] == 0):\n#         Task1_Dev['filename'][i] = DevPath + Task1_Dev['filename'][i].split('/')[-1]\n#     else:\n#         Task1_Dev['filename'][i] = DevPath + Task1_Dev['filename'][i].split('/')[-1]","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:38:02.165518Z","iopub.execute_input":"2023-12-30T16:38:02.166210Z","iopub.status.idle":"2023-12-30T16:38:02.778819Z","shell.execute_reply.started":"2023-12-30T16:38:02.166175Z","shell.execute_reply":"2023-12-30T16:38:02.777746Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def NERFeatureExtractor(text, nlp = nlp):\n    doc = nlp(text)\n    PersonCount = NORPCount = ORGCount = 0\n    for ent in doc.ents:\n        if ent.label_ == 'PERSON':\n            PersonCount = 1\n        elif ent.label_ == 'NORP':\n            NORPCount = 1\n        elif ent.label_ == 'ORG':\n            ORGCount = 1\n    NERFeaturesReady = {'PersonCount' : PersonCount, 'NORPCount' : NORPCount , 'ORGCount' : ORGCount}\n    return NERFeaturesReady","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:30:20.568535Z","iopub.execute_input":"2023-12-30T16:30:20.569407Z","iopub.status.idle":"2023-12-30T16:30:20.575645Z","shell.execute_reply.started":"2023-12-30T16:30:20.569371Z","shell.execute_reply":"2023-12-30T16:30:20.574680Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"%%time\n# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"summarization\", model=\"currentlyexhausted/flan-t5-summarizer\", device = \"cuda:0\", max_length = 70) \n\nfor i in range(len(Task1_Train)):\n    if(len(Task1_Train['text'][i].split()) > 100):\n        Task1_Train['text'][i] = pipe(Task1_Train['text'][i])[0]['summary_text']\n\nfor i in range(len(Task1_Test)):\n    if(len(Task1_Test['text'][i].split()) > 90):\n        Task1_Test['text'][i] = pipe(Task1_Test['text'][i])[0]['summary_text']","metadata":{"execution":{"iopub.status.busy":"2023-12-29T22:52:35.372566Z","iopub.execute_input":"2023-12-29T22:52:35.372929Z","iopub.status.idle":"2023-12-29T22:52:49.694554Z","shell.execute_reply.started":"2023-12-29T22:52:35.372900Z","shell.execute_reply":"2023-12-29T22:52:49.693667Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (1189 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 14 s, sys: 191 ms, total: 14.2 s\nWall time: 14.3 s\n","output_type":"stream"}]},{"cell_type":"code","source":"class MultiModalDataset(torch.utils.data.Dataset):\n    def __init__(self, images, texts, labels, image_transform=None, text_tokenizer=None):\n        self.images = images\n        self.texts = texts\n        self.labels = labels\n        self.image_transform = image_transform\n        self.text_tokenizer = text_tokenizer\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]\n        text = self.texts[idx]\n        label = self.labels[idx]\n        \n        image = Image.open(image)\n        \n        sample = {}\n        # Image preprocessing\n        if self.image_transform:\n            image = self.image_transform(image)\n\n        # Text preprocessing\n        if self.text_tokenizer:\n            text = self.text_tokenizer(text, return_tensors=\"pt\")['input_ids'].squeeze()\n\n        try:\n            sample[\"label\"] = int(label)\n            sample[\"text\"] = text\n            sample[\"image\"] = image\n        except Exception as e:\n            print(e)\n        \n        return sample","metadata":{"execution":{"iopub.status.busy":"2023-12-30T14:57:45.858671Z","iopub.execute_input":"2023-12-30T14:57:45.859045Z","iopub.status.idle":"2023-12-30T14:57:45.868113Z","shell.execute_reply.started":"2023-12-30T14:57:45.859016Z","shell.execute_reply":"2023-12-30T14:57:45.867005Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE = 224\n\nimage_train = transforms.Compose(\n    [\n        transforms.ToTensor(),\n    ]\n)\n\nMultimodaldataset = MultiModalDataset(Task1_Train['filename'].values, Task1_Train['text'].values, Task1_Train['label'].values, image_transform=image_train)\n\nMultimodaltestdataset = MultiModalDataset(Task1_Test['filename'].values, Task1_Test['text'].values, Task1_Test['label'].values, image_transform=image_train)","metadata":{"id":"SBYO97KcO_lo","execution":{"iopub.status.busy":"2023-12-30T14:58:52.823205Z","iopub.execute_input":"2023-12-30T14:58:52.823575Z","iopub.status.idle":"2023-12-30T14:58:52.831006Z","shell.execute_reply.started":"2023-12-30T14:58:52.823535Z","shell.execute_reply":"2023-12-30T14:58:52.829879Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom transformers import RobertaModel, RobertaTokenizer\n\nclass MultimodalClassifier(nn.Module):\n    def __init__(self, vision_model, text_model, num_classes):\n        super(MultimodalClassifier, self).__init__()\n        self.vision_model = vision_model\n        self.text_model = text_model\n        self.fc = nn.Sequential(\n            nn.Linear(1000 + text_model.config.hidden_size, 512),\n            nn.ReLU(),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, images, inputs):\n        \n        # Image processing with ViT\n        vision_output = self.vision_model(**images)\n        vision_embedding = vision_output.logits\n\n        # Text processing with RoBERTa\n        text_output = self.text_model(**inputs)\n        text_embedding = text_output.last_hidden_state[:, 0, :]\n\n        # Concatenate image and text embeddings\n        multimodal_embedding = torch.cat((vision_embedding, text_embedding), dim=1)\n\n        # Classification\n        linear_output = self.fc(multimodal_embedding)\n        \n        return F.log_softmax(linear_output, dim=1)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T14:59:49.361959Z","iopub.execute_input":"2023-12-30T14:59:49.362548Z","iopub.status.idle":"2023-12-30T14:59:49.372252Z","shell.execute_reply.started":"2023-12-30T14:59:49.362511Z","shell.execute_reply":"2023-12-30T14:59:49.371202Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\nfrom transformers import AutoModel, AutoTokenizer\n\n\n# Load pre-trained ViT and RoBERTa models\nvit_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\nvit_feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\nroberta_model = AutoModel.from_pretrained('GroNLP/HateBERT')\ntokenizer = AutoTokenizer.from_pretrained('GroNLP/HateBERT')\n\n# Create the multimodal classifier\nnum_classes = 3  # Set the number of classes according to your task\nmultimodal_classifier = MultimodalClassifier(vit_model, roberta_model, num_classes)\nmultimodal_classifier = multimodal_classifier.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T15:00:08.001740Z","iopub.execute_input":"2023-12-30T15:00:08.002420Z","iopub.status.idle":"2023-12-30T15:00:09.387589Z","shell.execute_reply.started":"2023-12-30T15:00:08.002382Z","shell.execute_reply":"2023-12-30T15:00:09.386557Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"## Specify the Hyper parameters \n\nBATCH_SIZE = 4\nNUM_LABELS = 3\nEPOCHS = 10\nLEARNING_RATE = 1e-5\n\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.enabled   = True","metadata":{"id":"iWnBZGueTKgJ","execution":{"iopub.status.busy":"2023-12-30T16:38:25.294197Z","iopub.execute_input":"2023-12-30T16:38:25.295078Z","iopub.status.idle":"2023-12-30T16:38:25.299764Z","shell.execute_reply.started":"2023-12-30T16:38:25.295044Z","shell.execute_reply":"2023-12-30T16:38:25.298779Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# split into training, validation and testing\n\ntrain_dataset, validation_dataset = torch.utils.data.random_split(dataset, [1642, 300])","metadata":{"id":"QrzyPhENPEr6","execution":{"iopub.status.busy":"2023-12-30T16:38:27.229666Z","iopub.execute_input":"2023-12-30T16:38:27.230021Z","iopub.status.idle":"2023-12-30T16:38:27.249725Z","shell.execute_reply.started":"2023-12-30T16:38:27.229992Z","shell.execute_reply":"2023-12-30T16:38:27.248838Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"## We call the dataloader class\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    pin_memory=True,\n    num_workers=4,\n    shuffle=True,\n    drop_last=True\n )\n\ntest_loader = torch.utils.data.DataLoader(\n    testdataset,\n    batch_size=1,\n    pin_memory=True,\n    num_workers=0,\n )\n\nval_loader = torch.utils.data.DataLoader(\n    validation_dataset,\n    batch_size=BATCH_SIZE,\n    pin_memory=True,\n    num_workers=4,\n    shuffle=True,\n    drop_last=True\n )\n\ndataloaders = {'Train': train_loader,'Test': test_loader, 'Val': val_loader}","metadata":{"id":"00ecG1syTHWG","execution":{"iopub.status.busy":"2023-12-30T16:38:30.335399Z","iopub.execute_input":"2023-12-30T16:38:30.335814Z","iopub.status.idle":"2023-12-30T16:38:30.342468Z","shell.execute_reply.started":"2023-12-30T16:38:30.335785Z","shell.execute_reply":"2023-12-30T16:38:30.341489Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n\n    def __init__(self):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(1792, 128)\n        self.fc2 = nn.Linear(128, 3)\n        self.dropout = nn.Dropout(0.25)\n\n    def forward(self, input):\n      linear_out = self.dropout(F.relu(self.fc1(input)))    \n      final_out = self.fc2(linear_out)    \n      return final_out","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:38:33.948589Z","iopub.execute_input":"2023-12-30T16:38:33.949447Z","iopub.status.idle":"2023-12-30T16:38:33.955482Z","shell.execute_reply.started":"2023-12-30T16:38:33.949396Z","shell.execute_reply":"2023-12-30T16:38:33.954569Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nfrom PIL import Image\n\n# Assuming you have already defined EPOCHS, model, dataloaders, criterion, optimizer, and device\n\ntrain_loss = []\ntrain_acc = []\n\nval_loss = []\nval_acc = []\n\nfor epoch in range(0, EPOCHS):\n    print('-' * 50)\n    print('Epoch {}/{}'.format(epoch + 1, EPOCHS))\n\n    for phase in ['Train', 'Val']:\n        batch_loss = 0.0  # live loss\n        batch_acc = 0.0  # live accuracy\n\n        y_pred = []\n        y_true = []\n\n        if phase == 'Train':\n            multimodal_classifier.train()\n        else:\n            multimodal_classifier.eval()\n\n        with tqdm(dataloaders[phase], unit=\"batch\", desc=phase) as tepoch:\n\n            for idx, batch in enumerate(tepoch):\n                labels = batch[\"label\"].to(device)\n                text = batch[\"text\"]\n\n                imgs = []\n\n                img_paths = batch[\"image\"]\n                \n                image = vit_feature_extractor(images = img_paths, return_tensors=\"pt\")\n                ImageInputs = {key: value.to(device) for key, value in image.items()}\n\n                inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n                inputs = {key: value.to(device) for key, value in inputs.items()}\n                \n                output = multimodal_classifier(ImageInputs, inputs)\n\n                # Ensure the model parameters are on the same device as the input data\n                output = output.to(device)\n                \n\n                loss = criterion(output, labels)\n\n                if phase == 'Train':\n                    # zero gradients\n                    optimizer.zero_grad()\n\n                    # Backward pass  (calculates the gradients)\n                    loss.backward()\n\n                    optimizer.step()  # Updates the weights\n\n                batch_loss += loss.item()\n\n                _, preds = output.data.max(1)\n\n                y_pred.extend(preds.tolist())\n                y_true.extend(labels.tolist())\n                \n                tepoch.set_postfix(loss=batch_loss / (idx + 1), accuracy=sum((y_pred[i] == y_true[i]) for i in range(len(y_true))) / len(y_true))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = []\nfiles = []\n\ni = 0\n\nwith tqdm(dataloaders['Test'], unit=\"batch\", desc=phase) as tepoch:\n  for idx, batch in enumerate(tepoch):\n    labels = batch[\"label\"].to(device)\n    text = batch[\"text\"]\n    imgs = []\n\n    img_paths = batch[\"image\"]\n\n    for path in img_paths:\n      i += 1\n      files.append((path.split('/')[-1][:5]))\n      imgs.append((Image.open(path)))\n    \n    image = vit_feature_extractor(images = img_paths, return_tensors=\"pt\")\n    ImageInputs = {key: value.to(device) for key, value in image.items()}\n\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n                \n    output = multimodal_classifier(ImageInputs, inputs)\n\n    _, preds = output.data.max(1)\n\n    y_pred.extend(preds.tolist())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\nwith open('submission.json', 'w') as json_file:\n    for key, value in sorted_dict.items():\n        predictionsDictionary = {'index': int(key), 'prediction': int(value)}\n        json.dump(predictionsDictionary, json_file)    \n        json_file.write('\\n')    ","metadata":{"execution":{"iopub.status.busy":"2023-12-30T16:17:32.834949Z","iopub.execute_input":"2023-12-30T16:17:32.835699Z","iopub.status.idle":"2023-12-30T16:17:32.846507Z","shell.execute_reply.started":"2023-12-30T16:17:32.835655Z","shell.execute_reply":"2023-12-30T16:17:32.845400Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"import json\n\nwith open('submission.json', 'w') as json_file:\n    for key, value in sorted_dict.items():\n        predictionsDictionary = {'index': int(key), 'prediction': int(value)}\n        json.dump(predictionsDictionary, json_file)    \n        json_file.write('\\n')    ","metadata":{"execution":{"iopub.status.busy":"2023-12-29T12:30:12.066048Z","iopub.execute_input":"2023-12-29T12:30:12.066481Z","iopub.status.idle":"2023-12-29T12:30:12.076378Z","shell.execute_reply.started":"2023-12-29T12:30:12.066452Z","shell.execute_reply":"2023-12-29T12:30:12.075505Z"},"trusted":true},"execution_count":145,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, accuracy_score","metadata":{"id":"t1XJ-0J0wXwq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(y_true,y_pred)","metadata":{"id":"o1qE9V8MxAKk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score(y_true, y_pred, average='macro')","metadata":{"id":"dFqhYozMwfrf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision_score(y_true, y_pred, average='macro')","metadata":{"id":"zKhy_AWLaFwY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recall_score(y_true, y_pred, average='macro')","metadata":{"id":"UwUR9c4DZGsx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imblearn.metrics import macro_averaged_mean_absolute_error \nmacro_averaged_mean_absolute_error(y_true, y_pred)","metadata":{"id":"8l_0xiHaaXl8"},"execution_count":null,"outputs":[]}]}