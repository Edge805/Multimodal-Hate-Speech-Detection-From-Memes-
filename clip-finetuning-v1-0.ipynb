{"metadata":{"colab":{"provenance":[]},"gpuClass":"premium","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7099911,"sourceType":"datasetVersion","datasetId":4037580},{"sourceId":7175708,"sourceType":"datasetVersion","datasetId":3950344},{"sourceId":7274397,"sourceType":"datasetVersion","datasetId":4217250},{"sourceId":7278472,"sourceType":"datasetVersion","datasetId":4092417},{"sourceId":7305441,"sourceType":"datasetVersion","datasetId":4036691}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re \nimport string \nimport warnings\nimport math\n\nwarnings.filterwarnings(\"ignore\")\n\n\nimport torch\nimport os\n\nfrom PIL import Image\n\nimport json\nimport matplotlib.pyplot as plt\n\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom time import sleep\nimport warnings\nfrom transformers import AdamW\n\nfrom transformers import CLIPProcessor, CLIPModel\nimport torch.nn.functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"62eDxz05BzPC","execution":{"iopub.status.busy":"2023-12-31T12:57:12.618426Z","iopub.execute_input":"2023-12-31T12:57:12.618791Z","iopub.status.idle":"2023-12-31T12:57:18.410028Z","shell.execute_reply.started":"2023-12-31T12:57:12.618763Z","shell.execute_reply":"2023-12-31T12:57:18.409086Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# read annotations file\n\nTask1_Train = pd.read_csv(\"/kaggle/input/multimodal-daatset-for-memes-detection/Task1Multimodal Ready.csv\")\nTask1_Dev = pd.read_csv(\"/kaggle/input/multimodal-daatset-for-memes-detection/stA_eval(1).csv\")\nTask1_Test = pd.read_csv(\"/kaggle/input/multimodal-daatset-for-memes-detection/stA_test.csv\")\n\nTask1_Train.dropna(inplace=True)","metadata":{"id":"AunCHFubBxUO","execution":{"iopub.status.busy":"2023-12-31T12:57:24.238508Z","iopub.execute_input":"2023-12-31T12:57:24.239324Z","iopub.status.idle":"2023-12-31T12:57:24.311466Z","shell.execute_reply.started":"2023-12-31T12:57:24.239288Z","shell.execute_reply":"2023-12-31T12:57:24.310586Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"Task1_Train['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T12:57:26.032044Z","iopub.execute_input":"2023-12-31T12:57:26.032930Z","iopub.status.idle":"2023-12-31T12:57:26.050644Z","shell.execute_reply.started":"2023-12-31T12:57:26.032897Z","shell.execute_reply":"2023-12-31T12:57:26.049530Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"label\n1    1942\n0    1658\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"def remove_emojis(text):\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n                               u\"\\U0001F700-\\U0001F77F\"  # Alphanumeric Supplement\n                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n                               u\"\\U000024C2-\\U0001F251\" \n                               \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_punctuation(input_string):\n    translator = str.maketrans('', '', string.punctuation)\n    \n    result = input_string.translate(translator)\n    \n    return result\n\ndef remove_non_english_chars(input_string):\n    translation_table = dict.fromkeys(\n        i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('C')\n    )\n\n    result = input_string.translate(translation_table)\n\n    return result\n\ndef correct_spelling(input_text):\n    try:\n        # Create a SpellChecker object\n        spell = SpellChecker()\n\n        # Split the input text into words\n        words = input_text.split()\n\n        # Find misspelled words\n        misspelled = spell.unknown(words)\n\n        # Correct misspelled words\n        corrected_text = \" \".join(spell.correction(word) if word in misspelled else word for word in words)\n\n        return corrected_text\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return input_text\n    \nhyperlink_pattern = r'https?://\\S+|www\\.\\S+'\n\ndef PreProcessTweets(tweet):\n  tweet2 = str(tweet)\n  tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n  tweet2 = re.sub(r'#([^\\s]+)', '', tweet2)\n  tweet2 = tweet2.replace(\"LINK\",\"\")\n  tweet2 = tweet2.replace(\"&amp\",\"\") \n  tweet2 = re.sub(r'@', '', tweet2)\n  tweet2 = tweet2.replace(\"\\n\",\"\")\n  tweet2 = re.sub(r'_', ' ', tweet2)\n  tweet2 = tweet2.replace(\"UN\",\"United Nations \")\n  tweet2 = re.sub(hyperlink_pattern, '', tweet2)\n  tweet2 = tweet2.replace(\":\",\"\") \n  tweet2 = re.sub('‼‼‼','', tweet2)\n  tweet2 = remove_emojis(tweet2)\n  tweet2 = remove_punctuation(tweet2)\n  tweet2 = re.sub(' +', ' ',tweet2)\n  tweet2 = tweet2.replace(\"youve\",\"you have\")\n  tweet2 = tweet2.replace(\"govt\",\"Government\")\n#   tweet2 = correct_spelling(tweet2)\n  return tweet2\n\nTask1_Train['text'] = Task1_Train['text'].apply(lambda x: PreProcessTweets(x))\nTask1_Dev['text'] = Task1_Dev['text'].apply(lambda x: PreProcessTweets(x))\nTask1_Test['text'] = Task1_Test['text'].apply(lambda x: PreProcessTweets(x))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T12:57:27.853304Z","iopub.execute_input":"2023-12-31T12:57:27.853662Z","iopub.status.idle":"2023-12-31T12:57:28.180561Z","shell.execute_reply.started":"2023-12-31T12:57:27.853634Z","shell.execute_reply":"2023-12-31T12:57:28.179592Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"ImagePathTest = '/kaggle/input/subtaska-hate-speach-detection/subtaskA-20231211T122329Z-001/subtaskA/'\n\nfor i in range(len(Task1_Test)):\n    Task1_Test['filename'][i] = ImagePathTest + Task1_Test['filename'][i].split('/')[-1]","metadata":{"execution":{"iopub.status.busy":"2023-12-31T12:57:31.302321Z","iopub.execute_input":"2023-12-31T12:57:31.303108Z","iopub.status.idle":"2023-12-31T12:57:31.417010Z","shell.execute_reply.started":"2023-12-31T12:57:31.303073Z","shell.execute_reply":"2023-12-31T12:57:31.415712Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Indvidual = '/kaggle/input/subtask-b/subTaskB-20231121T151628Z-001/subTaskB/Individual/'\n# Community = '/kaggle/input/subtask-b/subTaskB-20231121T151628Z-001/subTaskB/Community/'\n# Organization = '/kaggle/input/subtask-b/subTaskB-20231121T151628Z-001/subTaskB/Organization/'\n\nHatePath = '/kaggle/input/task-a/subTaskA-20231121T171317Z-001/subTaskA/Hate Speech/'\nNonHatePath = '/kaggle/input/task-a/subTaskA-20231121T171317Z-001/subTaskA/No Hate Speech/'\n\nfor i in range(len(Task1_Train)):\n    if(Task1_Train['label'][i] == 0):\n        Task1_Train['filename'][i] = NonHatePath + Task1_Train['filename'][i].split('/')[-1]\n    elif(Task1_Train['label'][i] == 1):\n        Task1_Train['filename'][i] = HatePath + Task1_Train['filename'][i].split('/')[-1]\n#     elif(Task1_Train['label'][i] == 2):\n#         Task1_Train['filename'][i] = Organization + Task1_Train['filename'][i].split('/')[-1]\n \nDevPath = '/kaggle/input/subtaska-hate-speach-detection/subtaskA-20231111T125143Z-001/subtaskA/'\n\nfor i in range(len(Task1_Dev)):\n    if(Task1_Dev['label'][i] == 0):\n        Task1_Dev['filename'][i] = DevPath + Task1_Dev['filename'][i].split('/')[-1]\n    else:\n        Task1_Dev['filename'][i] = DevPath + Task1_Dev['filename'][i].split('/')[-1]","metadata":{"execution":{"iopub.status.busy":"2023-12-31T12:57:32.828168Z","iopub.execute_input":"2023-12-31T12:57:32.828605Z","iopub.status.idle":"2023-12-31T12:57:34.221795Z","shell.execute_reply.started":"2023-12-31T12:57:32.828572Z","shell.execute_reply":"2023-12-31T12:57:34.220487Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"Task1_Train = pd.concat([Task1_Train,Task1_Dev],axis=0)\nTask1_Train = Task1_Train.reset_index()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T12:57:36.028893Z","iopub.execute_input":"2023-12-31T12:57:36.029729Z","iopub.status.idle":"2023-12-31T12:57:36.040379Z","shell.execute_reply.started":"2023-12-31T12:57:36.029667Z","shell.execute_reply":"2023-12-31T12:57:36.039575Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"Task1_Train['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T12:57:37.525789Z","iopub.execute_input":"2023-12-31T12:57:37.526148Z","iopub.status.idle":"2023-12-31T12:57:37.534059Z","shell.execute_reply.started":"2023-12-31T12:57:37.526117Z","shell.execute_reply":"2023-12-31T12:57:37.532937Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"label\n1    2185\n0    1858\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import pipeline\n\npipe = pipeline(\"summarization\", model=\"currentlyexhausted/flan-t5-summarizer\", device = \"cuda:0\", max_length = 70) \n\n# for i in range(len(Task1_Train)):\n#     if(len(Task1_Train['text'][i].split()) > 50):\n#         Task1_Train['text'][i] = pipe(Task1_Train['text'][i])[0]['summary_text']\n\nfor i in range(len(Task1_Test)):\n    if(len(Task1_Test['text'][i].split()) > 50):\n        Task1_Test['text'][i] = pipe(Task1_Test['text'][i])[0]['summary_text']\n                                                       \n    #if(org_count > person_count):\n        #Task1_Dev['label'][i] = 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DATASET\n\nclass Dataset(torch.utils.data.Dataset):\n    \"\"\"\n    This is our custom dataset class which will load the text and their corresponding labels into Pytorch tensors\n    \"\"\"\n    def __init__(self, paths, text, labels):\n        self.labels = labels\n        self.texts = text\n        self.paths = paths\n\n    def __getitem__(self, idx):\n        sample = {}\n        img_path = self.paths[idx]\n\n        text = self.texts[idx]\n\n        label = self.labels[idx]\n\n        try:\n            sample[\"label\"] = int(label)\n            sample[\"text\"] = text\n            sample[\"image\"] = img_path\n        except Exception as e:\n            print(e)\n        \n        return sample\n    \n    def __len__(self):\n        return len(self.labels)","metadata":{"id":"Dc6YxJM_CZEu","execution":{"iopub.status.busy":"2023-12-31T12:57:59.654151Z","iopub.execute_input":"2023-12-31T12:57:59.654647Z","iopub.status.idle":"2023-12-31T12:57:59.663621Z","shell.execute_reply.started":"2023-12-31T12:57:59.654610Z","shell.execute_reply":"2023-12-31T12:57:59.662490Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"Task1_Test['label'] = 0\n\ndataset = Dataset(Task1_Train['filename'].values, Task1_Train['text'].values, Task1_Train['label'].values)\n\ntestdataset = Dataset(Task1_Test['filename'].values, Task1_Test['text'].values, Task1_Test['label'].values)","metadata":{"id":"SBYO97KcO_lo","execution":{"iopub.status.busy":"2023-12-31T12:58:01.463445Z","iopub.execute_input":"2023-12-31T12:58:01.463830Z","iopub.status.idle":"2023-12-31T12:58:01.470645Z","shell.execute_reply.started":"2023-12-31T12:58:01.463800Z","shell.execute_reply":"2023-12-31T12:58:01.469202Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"len(Task1_Train)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T12:58:03.014972Z","iopub.execute_input":"2023-12-31T12:58:03.015609Z","iopub.status.idle":"2023-12-31T12:58:03.021337Z","shell.execute_reply.started":"2023-12-31T12:58:03.015576Z","shell.execute_reply":"2023-12-31T12:58:03.020432Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"4043"},"metadata":{}}]},{"cell_type":"code","source":"# split into training, validation and testing\n\ntrain_dataset, validation_dataset = torch.utils.data.random_split(dataset, [3643, 400])","metadata":{"id":"QrzyPhENPEr6","execution":{"iopub.status.busy":"2023-12-31T12:59:08.007335Z","iopub.execute_input":"2023-12-31T12:59:08.007679Z","iopub.status.idle":"2023-12-31T12:59:08.030569Z","shell.execute_reply.started":"2023-12-31T12:59:08.007654Z","shell.execute_reply":"2023-12-31T12:59:08.029712Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass Model(nn.Module):\n\n    def __init__(self):\n        super(Model, self).__init__()\n\n        self.fc1 = nn.Linear(1792, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 128)\n        self.fc4 = nn.Linear(128, 3)  # Output layer for classification\n\n        self.dropout = nn.Dropout(0.25)\n\n    def forward(self, input):\n        linear_out1 = self.dropout(F.relu(self.fc1(input)))\n        linear_out2 = self.dropout(F.relu(self.fc2(linear_out1)))\n        linear_out3 = self.dropout(F.relu(self.fc3(linear_out2)))\n        final_out = self.fc4(linear_out3)  # Output layer with no activation for now\n\n        return final_out","metadata":{"execution":{"iopub.status.busy":"2023-12-31T12:59:10.193134Z","iopub.execute_input":"2023-12-31T12:59:10.193549Z","iopub.status.idle":"2023-12-31T12:59:10.202032Z","shell.execute_reply.started":"2023-12-31T12:59:10.193516Z","shell.execute_reply":"2023-12-31T12:59:10.200859Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"## Specify the Hyper parameters \n\nBATCH_SIZE = 8\nNUM_LABELS = 2\nEPOCHS = 10\nLEARNING_RATE = 1e-5\n\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.enabled   = True","metadata":{"id":"iWnBZGueTKgJ","execution":{"iopub.status.busy":"2023-12-31T12:59:15.908684Z","iopub.execute_input":"2023-12-31T12:59:15.909050Z","iopub.status.idle":"2023-12-31T12:59:15.914038Z","shell.execute_reply.started":"2023-12-31T12:59:15.909020Z","shell.execute_reply":"2023-12-31T12:59:15.912924Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"## We call the dataloader class\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    pin_memory=True,\n    num_workers=4,\n    shuffle=True,\n    drop_last=True\n )\n\ntest_loader = torch.utils.data.DataLoader(\n    testdataset,\n    batch_size=1,\n    pin_memory=True,\n    num_workers=0,\n )\n\nval_loader = torch.utils.data.DataLoader(\n    validation_dataset,\n    batch_size=BATCH_SIZE,\n    pin_memory=True,\n    num_workers=4,\n    shuffle=True,\n    drop_last=True\n )\n\ndataloaders = {'Train': train_loader,'Test': test_loader, 'Val': val_loader}","metadata":{"id":"00ecG1syTHWG","execution":{"iopub.status.busy":"2023-12-31T12:59:18.038057Z","iopub.execute_input":"2023-12-31T12:59:18.038950Z","iopub.status.idle":"2023-12-31T12:59:18.045778Z","shell.execute_reply.started":"2023-12-31T12:59:18.038914Z","shell.execute_reply":"2023-12-31T12:59:18.044615Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"Task1_Train['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T12:59:20.437628Z","iopub.execute_input":"2023-12-31T12:59:20.438375Z","iopub.status.idle":"2023-12-31T12:59:20.446171Z","shell.execute_reply.started":"2023-12-31T12:59:20.438339Z","shell.execute_reply":"2023-12-31T12:59:20.445278Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"label\n1    2185\n0    1858\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"clip = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n\nclip = clip.to(device)\n\nmodel = Model().to(device)\n\n#optimizer\noptimizer = AdamW(model.parameters(), lr = LEARNING_RATE, eps=1e-8)\n\n#Loss function\ncriterion = nn.CrossEntropyLoss()","metadata":{"id":"iRC2KlDeeFGd","execution":{"iopub.status.busy":"2023-12-31T12:59:21.841183Z","iopub.execute_input":"2023-12-31T12:59:21.841599Z","iopub.status.idle":"2023-12-31T13:00:12.187864Z","shell.execute_reply.started":"2023-12-31T12:59:21.841569Z","shell.execute_reply":"2023-12-31T13:00:12.187035Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.52k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c78ba9d6a8b14b178a1efda23d2c527a"}},"metadata":{}},{"name":"stderr","text":"`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"380fa0c0673243959db305f193229334"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcd385547603463a9c5f36cc953474fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cca794adf63c486785d4ed97cbe5eccc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c0990b279054147b28a72bb7105c09b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"827c98aa8c8b47e68e45a7e4ffc96b45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cde309eb51ca40dd904ff8a7f39b68f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47ffbae781654efc9136ea92432e53fb"}},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nfrom PIL import Image\n\ntrain_loss = []\ntrain_acc = []\n\nval_loss = []\nval_acc = []\n\nfor epoch in range(0, EPOCHS):\n    print('-' * 50)\n    print('Epoch {}/{}'.format(epoch + 1, EPOCHS))\n\n    for phase in ['Train', 'Val']:\n        batch_loss = 0.0  # live loss\n        batch_acc = 0.0  # live accuracy\n\n        y_pred = []\n        y_true = []\n\n        if phase == 'Train':\n            model.train()\n        else:\n            model.eval()\n\n        with tqdm(dataloaders[phase], unit=\"batch\", desc=phase) as tepoch:\n\n            for idx, batch in enumerate(tepoch):\n                labels = batch[\"label\"].to(device)\n                text = batch[\"text\"]\n\n                imgs = []\n\n                img_paths = batch[\"image\"]\n\n                for path in img_paths:\n                    imgs.append((Image.open(path)))\n\n                inputs = processor(text=text, images=imgs, return_tensors=\"pt\", padding=True, truncation=True)\n                inputs = {key: value.to(device) for key, value in inputs.items()}\n                clip_output = clip(**inputs)\n\n                img_embed = clip_output.vision_model_output.pooler_output\n                text_embed = clip_output.text_model_output.pooler_output\n\n                concat = torch.cat((text_embed, img_embed), 1).to(device)\n\n                output = model(concat)\n\n                output = output.to(device)\n\n                loss = criterion(output, labels)\n\n                if phase == 'Train':\n                    optimizer.zero_grad()\n\n                    loss.backward()\n\n                    optimizer.step()  \n\n                batch_loss += loss.item()\n\n                _, preds = output.data.max(1)\n\n                y_pred.extend(preds.tolist())\n                y_true.extend(labels.tolist())\n                \n                tepoch.set_postfix(loss=batch_loss / (idx + 1), accuracy=sum((y_pred[i] == y_true[i]) for i in range(len(y_true))) / len(y_true))","metadata":{"id":"erDQFfVOecZg","execution":{"iopub.status.busy":"2023-12-31T13:00:45.494246Z","iopub.execute_input":"2023-12-31T13:00:45.494619Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"--------------------------------------------------\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Train:   0%|          | 0/455 [00:00<?, ?batch/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nTrain: 100%|██████████| 455/455 [07:28<00:00,  1.02batch/s, accuracy=0.583, loss=0.865]\nVal:   0%|          | 0/50 [00:00<?, ?batch/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nVal: 100%|██████████| 50/50 [00:25<00:00,  1.96batch/s, accuracy=0.78, loss=0.633] \n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Train:   0%|          | 0/455 [00:00<?, ?batch/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nTrain: 100%|██████████| 455/455 [07:16<00:00,  1.04batch/s, accuracy=0.743, loss=0.579]\nVal:   0%|          | 0/50 [00:00<?, ?batch/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nVal: 100%|██████████| 50/50 [00:23<00:00,  2.09batch/s, accuracy=0.812, loss=0.472]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Train:   0%|          | 0/455 [00:00<?, ?batch/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nTrain: 100%|██████████| 455/455 [07:16<00:00,  1.04batch/s, accuracy=0.775, loss=0.505]\nVal:   0%|          | 0/50 [00:00<?, ?batch/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nVal: 100%|██████████| 50/50 [00:23<00:00,  2.08batch/s, accuracy=0.815, loss=0.44] \n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Train:   0%|          | 0/455 [00:00<?, ?batch/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nTrain: 100%|██████████| 455/455 [07:17<00:00,  1.04batch/s, accuracy=0.798, loss=0.472]\nVal:   0%|          | 0/50 [00:00<?, ?batch/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nVal: 100%|██████████| 50/50 [00:24<00:00,  2.08batch/s, accuracy=0.815, loss=0.433]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Train:   0%|          | 0/455 [00:00<?, ?batch/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nTrain: 100%|██████████| 455/455 [07:16<00:00,  1.04batch/s, accuracy=0.805, loss=0.455]\nVal:   0%|          | 0/50 [00:00<?, ?batch/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nVal: 100%|██████████| 50/50 [00:24<00:00,  2.08batch/s, accuracy=0.82, loss=0.415] \n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"Train:   0%|          | 0/455 [00:00<?, ?batch/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nTrain: 100%|██████████| 455/455 [07:16<00:00,  1.04batch/s, accuracy=0.815, loss=0.437]\nVal:   0%|          | 0/50 [00:00<?, ?batch/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nVal: 100%|██████████| 50/50 [00:23<00:00,  2.19batch/s, accuracy=0.833, loss=0.42] TOKENIZERS_PARALLELISM=(true | false)\nVal: 100%|██████████| 50/50 [00:24<00:00,  2.08batch/s, accuracy=0.833, loss=0.42]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Train:   0%|          | 0/455 [00:00<?, ?batch/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nTrain:  50%|████▉     | 226/455 [03:37<03:40,  1.04batch/s, accuracy=0.826, loss=0.419]","output_type":"stream"}]},{"cell_type":"code","source":"y_pred = []\nfiles = []\n\ni = 0\n\nwith tqdm(dataloaders['Test'], unit=\"batch\", desc=phase) as tepoch:\n  for idx, batch in enumerate(tepoch):\n    labels = batch[\"label\"].to(device)\n    text = batch[\"text\"]\n\n    imgs = []\n\n    img_paths = batch[\"image\"]\n\n    for path in img_paths:\n      i += 1\n      files.append((path.split('/')[-1][:5]))\n      imgs.append((Image.open(path)))\n    \n    inputs = processor(text=text, images=imgs, return_tensors=\"pt\", padding=True, truncation = True)\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n    clip_output = clip(**inputs)\n\n    img_embed = clip_output.vision_model_output.pooler_output\n\n    text_embed = clip_output.text_model_output.pooler_output\n\n    concat = torch.cat((text_embed, img_embed), 1)\n\n    output = model(concat)\n\n    _, preds = output.data.max(1)\n\n    y_pred.extend(preds.tolist())","metadata":{"id":"bEqqdS4qNpOf","execution":{"iopub.status.busy":"2023-12-31T11:22:54.576826Z","iopub.execute_input":"2023-12-31T11:22:54.577979Z","iopub.status.idle":"2023-12-31T11:23:12.820657Z","shell.execute_reply.started":"2023-12-31T11:22:54.577931Z","shell.execute_reply":"2023-12-31T11:23:12.819477Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"Val: 100%|██████████| 242/242 [00:18<00:00, 13.28batch/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"pred = {}\nfor i in range(len(files)):\n    pred[files[i]] = y_pred[i]\n    \nassert len(sorted_dict) == len(Task1_Test), \"Check the inference loop.\"","metadata":{"execution":{"iopub.status.busy":"2023-12-31T11:23:39.066490Z","iopub.execute_input":"2023-12-31T11:23:39.066875Z","iopub.status.idle":"2023-12-31T11:23:39.071859Z","shell.execute_reply.started":"2023-12-31T11:23:39.066845Z","shell.execute_reply":"2023-12-31T11:23:39.070893Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"import json\n\nwith open('submission.json', 'w') as json_file:\n    for key, value in sorted_dict.items():\n        predictionsDictionary = {'index': int(key), 'prediction': int(value)}\n        json.dump(predictionsDictionary, json_file)    \n        json_file.write('\\n')    ","metadata":{"execution":{"iopub.status.busy":"2023-12-31T11:23:42.540521Z","iopub.execute_input":"2023-12-31T11:23:42.540912Z","iopub.status.idle":"2023-12-31T11:23:42.545569Z","shell.execute_reply.started":"2023-12-31T11:23:42.540880Z","shell.execute_reply":"2023-12-31T11:23:42.544635Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, accuracy_score","metadata":{"id":"t1XJ-0J0wXwq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(y_true,y_pred)","metadata":{"id":"o1qE9V8MxAKk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score(y_true, y_pred, average='macro')","metadata":{"id":"dFqhYozMwfrf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision_score(y_true, y_pred, average='macro')","metadata":{"id":"zKhy_AWLaFwY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recall_score(y_true, y_pred, average='macro')","metadata":{"id":"UwUR9c4DZGsx"},"execution_count":null,"outputs":[]}]}